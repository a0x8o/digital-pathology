{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d245291-3aaf-466b-a7a6-c8aca75404a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "This is added to Workspace path to circumvent encountered issues (tested post Dec2024) running [RUNME](https://e2-demo-field-eng.cloud.databricks.com/editor/notebooks/3477183817543754?o=1444828305810485#command/1177265714186631). The `__init__.py` code is taken from [solacc/companion/\\__init\\__.py](https://github.com/databricks-industry-solutions/notebook-solution-companion/blob/f7e381d77675b29c2d3f9d377a528ceaf2255f23/solacc/companion/__init__.py) <!-- link wrt the PR update -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2c96363-2323-4d29-b45e-fd96249bec04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "from dbacademy.dbrest import DBAcademyRestClient\n",
    "from dbruntime.display import displayHTML\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import hashlib\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from databricks.sdk.service.jobs import JobSettings, CreateJob\n",
    "from databricks.sdk.service.pipelines import EditPipeline, CreatePipeline\n",
    "from databricks.sdk.service.compute import CreateCluster\n",
    "\n",
    "def init_locals():\n",
    "\n",
    "    # noinspection PyGlobalUndefined\n",
    "    global spark, sc, dbutils\n",
    "\n",
    "    try: spark\n",
    "    except NameError:spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    try: sc\n",
    "    except NameError: sc = spark.sparkContext\n",
    "\n",
    "    try: dbutils\n",
    "    except NameError:\n",
    "        if spark.conf.get(\"spark.databricks.service.client.enabled\") == \"true\":\n",
    "            from pyspark.dbutils import DBUtils\n",
    "            dbutils = DBUtils(spark)\n",
    "        else:\n",
    "            import IPython\n",
    "            dbutils = IPython.get_ipython().user_ns[\"dbutils\"]\n",
    "\n",
    "    return sc, spark, dbutils\n",
    "\n",
    "\n",
    "sc, spark, dbutils = init_locals()\n",
    "\n",
    "class NotebookSolutionCompanion():\n",
    "  \"\"\"\n",
    "  A class to provision companion assets for a notebook-based solution, includingn job, cluster(s), DLT pipeline(s) and DBSQL dashboard(s)\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.w = self.get_workspace_client()\n",
    "    self.solution_code_name = self.get_notebook_dir().split('/')[-1]\n",
    "    self.solacc_path = self.get_notebook_dir()\n",
    "    hash_code = hashlib.sha256(self.solacc_path.encode()).hexdigest()\n",
    "    self.job_name = f\"[RUNNER] {self.solution_code_name} | {hash_code}\" # use hash to differentiate solutions deployed to different paths\n",
    "    self.client = DBAcademyRestClient() # part of this code uses dbacademy rest client as the SDK migration work is ongoing\n",
    "    self.workspace_url = self.get_workspace_url()\n",
    "    self.print_html = int(spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\").split(\".\")[0].split(\"__\")[-1]) >= 11 # below DBR 11, html print is not supported\n",
    "    self.username = self.get_username()\n",
    "    self.cloud = self.get_cloud()\n",
    "  \n",
    "  def get_cloud(self) -> str:\n",
    "    if self.w.config.is_azure:\n",
    "      return \"MSA\"\n",
    "    elif self.w.config.is_aws:\n",
    "      return \"AWS\"\n",
    "    elif self.w.config.is_gcp:\n",
    "      return \"GCP\"\n",
    "    else: \n",
    "      raise NotImplementedError\n",
    "\n",
    "  @staticmethod\n",
    "  def get_workspace_client() -> WorkspaceClient: \n",
    "    ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "    DATABRICKS_TOKEN = ctx.apiToken().getOrElse(None)\n",
    "    DATABRICKS_URL = ctx.apiUrl().getOrElse(None)\n",
    "    return WorkspaceClient(host=DATABRICKS_URL, token=DATABRICKS_TOKEN)\n",
    "\n",
    "  def get_username(self) -> str:\n",
    "    return self.w.current_user.me().user_name\n",
    "      \n",
    "  @staticmethod\n",
    "  def get_workspace_url() -> str:\n",
    "    try:\n",
    "        url = spark.conf.get('spark.databricks.workspaceUrl') # wrap this in try because this config went missing in GCP in July 2023\n",
    "    except:\n",
    "        url = \"\"\n",
    "    return url\n",
    "\n",
    "  @staticmethod\n",
    "  def get_notebook_dir() -> str:\n",
    "    notebook_path = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
    "    return \"/\".join(notebook_path.split(\"/\")[:-1])\n",
    "  \n",
    "  @staticmethod\n",
    "  def convert_job_cluster_to_cluster(job_cluster_params):\n",
    "    params = job_cluster_params[\"new_cluster\"]\n",
    "    params[\"cluster_name\"] = f\"\"\"{job_cluster_params[\"job_cluster_key\"]}\"\"\"\n",
    "    params[\"autotermination_minutes\"] = 15 # adding a default autotermination as best practice\n",
    "    return params\n",
    "\n",
    "  def create_or_update_job_by_name(self, params):\n",
    "    \"\"\"Look up the companion job by name and resets it with the given param and return job id; create a new job if a job with that name does not exist\"\"\"\n",
    "    # job_found = self.client.jobs().get_by_name(params[\"name\"])\n",
    "    job_found = list(self.w.jobs.list(name=params[\"name\"]))\n",
    "    if job_found: \n",
    "      job_id = job_found[0].job_id\n",
    "      # reset_params = {\"job_id\": job_id,\n",
    "      #                \"new_settings\": params}\n",
    "      reset_job_settings = JobSettings().from_dict(params)\n",
    "      # json_response = self.client.execute_post_json(f\"{self.client.endpoint}/api/2.1/jobs/reset\", reset_params) # returns {} if status is 200\n",
    "      self.w.jobs.reset(job_id, reset_job_settings)\n",
    "      # assert json_response == {}, \"Job reset returned non-200 status\"\n",
    "      \n",
    "      if self.print_html:\n",
    "          displayHTML(f\"\"\"Reset the <a href=\"/#job/{job_id}/tasks\" target=\"_blank\">{params[\"name\"]}</a> job to original definition\"\"\")\n",
    "      else:\n",
    "          print(f\"\"\"Reset the {params[\"name\"]} job at: {self.workspace_url}/#job/{job_id}/tasks\"\"\")\n",
    "          \n",
    "    else:\n",
    "      # json_response = self.client.execute_post_json(f\"{self.client.endpoint}/api/2.1/jobs/create\", params)\n",
    "      create_job_request = CreateJob().from_dict(params)\n",
    "      job_id = self.w.jobs.create(request=create_job_request).job_id\n",
    "      if self.print_html:\n",
    "          displayHTML(f\"\"\"Created <a href=\"/#job/{job_id}/tasks\" target=\"_blank\">{params[\"name\"]}</a> job\"\"\")\n",
    "      else:\n",
    "          print(f\"\"\"Created {params[\"name\"]} job at: {self.workspace_url}/#job/{job_id}/tasks\"\"\")\n",
    "          \n",
    "    return job_id\n",
    "  \n",
    "  # Note these functions assume that names for solacc jobs/cluster/pipelines are unique, which is guaranteed if solacc jobs/cluster/pipelines are created from this class only\n",
    "  def create_or_update_pipeline_by_name(self, dlt_config_table, pipeline_name, dlt_definition_dict, spark):\n",
    "    \"\"\"Look up a companion pipeline by name and edit with the given param and return pipeline id; create a new pipeline if a pipeline with that name does not exist\"\"\"\n",
    "    # pipeline_found = self.client.pipelines.get_by_name(pipeline_name)\n",
    "    pipeline_found = list(self.w.pipelines.list_pipelines(filter=f\"name LIKE '{pipeline_name}'\"))\n",
    "      \n",
    "    if pipeline_found:\n",
    "        pipeline_id = pipeline_found[0].pipeline_id\n",
    "        dlt_definition_dict['pipeline_id'] = pipeline_id\n",
    "        # self.client.execute_put_json(f\"{self.client.endpoint}/api/2.0/pipelines/{pipeline_id}\", dlt_definition_dict)\n",
    "        request = EditPipeline(pipeline_id = pipeline_id).from_dict(dlt_definition_dict)\n",
    "        self.w.pipelines.update(request=request, pipeline_id=pipeline_id)\n",
    "    else:\n",
    "        # response = self.client.pipelines().create_from_dict(dlt_definition_dict)\n",
    "        request = CreatePipeline().from_dict(dlt_definition_dict)\n",
    "        pipeline_id = self.w.pipelines.create(request=request).pipeline_id\n",
    "        \n",
    "    return pipeline_id\n",
    "  \n",
    "  def create_or_update_cluster_by_name(self, params):\n",
    "      \"\"\"Look up a companion cluster by name and edit with the given param and return cluster id; create a new cluster if a cluster with that name does not exist\"\"\"\n",
    "      \n",
    "      def edit_cluster(client, cluster_id, params):\n",
    "        \"\"\"Wait for a cluster to be in editable states and edit it to the specified params\"\"\"\n",
    "        cluster_state = client.execute_get_json(f\"{client.endpoint}/api/2.0/clusters/get?cluster_id={cluster_id}\")[\"state\"]\n",
    "        while cluster_state not in (\"RUNNING\", \"TERMINATED\"): # cluster edit only works in these states; all other states will eventually turn into those two, so we wait and try later\n",
    "          time.sleep(30) \n",
    "          cluster_state = client.execute_get_json(f\"{client.endpoint}/api/2.0/clusters/get?cluster_id={cluster_id}\")[\"state\"]\n",
    "        json_response = client.execute_post_json(f\"{client.endpoint}/api/2.0/clusters/edit\", params) # returns {} if status is 200\n",
    "        assert json_response == {}, \"Cluster edit returned non-200 status\"\n",
    "      \n",
    "      clusters = self.client.execute_get_json(f\"{self.client.endpoint}/api/2.0/clusters/list\")[\"clusters\"]\n",
    "      clusters_matched = list(filter(lambda cluster: params[\"cluster_name\"] == cluster[\"cluster_name\"], clusters))\n",
    "      cluster_id = clusters_matched[0][\"cluster_id\"] if len(clusters_matched) == 1 else None\n",
    "      if cluster_id: \n",
    "        params[\"cluster_id\"] = cluster_id\n",
    "        edit_cluster(self.client, cluster_id, params)\n",
    "        if self.print_html:\n",
    "          displayHTML(f\"\"\"Reset the <a href=\"/#setting/clusters/{cluster_id}/configuration\" target=\"_blank\">{params[\"cluster_name\"]}</a> cluster to original definition\"\"\")\n",
    "        else:\n",
    "          print(f\"\"\"Reset the {params[\"cluster_name\"]} cluster at: {self.workspace_url}/#setting/clusters/{cluster_id}/configuration\"\"\")\n",
    "          \n",
    "        \n",
    "        \n",
    "      else:\n",
    "        json_response = self.client.execute_post_json(f\"{self.client.endpoint}/api/2.0/clusters/create\", params)\n",
    "        cluster_id = json_response[\"cluster_id\"]\n",
    "        if self.print_html:\n",
    "          displayHTML(f\"\"\"Created <a href=\"/#setting/clusters/{cluster_id}/configuration\" target=\"_blank\">{params[\"cluster_name\"]}</a> cluster\"\"\")\n",
    "        else:\n",
    "          print(f\"\"\"Created {params[\"cluster_name\"]} cluster at: {self.workspace_url}/#setting/clusters/{cluster_id}/configuration\"\"\")\n",
    "        \n",
    "      return cluster_id\n",
    "\n",
    "  def customize_cluster_json(self, input_json):\n",
    "    node_type_id_dict = copy.deepcopy(input_json[\"node_type_id\"]) \n",
    "    input_json[\"node_type_id\"] = node_type_id_dict[self.cloud]\n",
    "    if self.cloud == \"AWS\": \n",
    "      input_json[\"aws_attributes\"] = {\n",
    "                        \"availability\": \"ON_DEMAND\",\n",
    "                        \"zone_id\": \"auto\"\n",
    "                    }\n",
    "    if self.cloud == \"MSA\": \n",
    "      input_json[\"azure_attributes\"] = {\n",
    "                        \"availability\": \"ON_DEMAND_AZURE\",\n",
    "                        \"zone_id\": \"auto\"\n",
    "                    }\n",
    "    if self.cloud == \"GCP\": \n",
    "      input_json[\"gcp_attributes\"] = {\n",
    "                        \"use_preemptible_executors\": False\n",
    "                    }\n",
    "    return input_json\n",
    "    \n",
    "  @staticmethod\n",
    "  def customize_job_json(input_json, job_name, solacc_path, cloud):\n",
    "    if \"name\" not in input_json:\n",
    "      input_json[\"name\"] = job_name\n",
    "\n",
    "    for i, _ in enumerate(input_json[\"tasks\"]):\n",
    "      if \"notebook_task\" in input_json[\"tasks\"][i]:\n",
    "        notebook_name = input_json[\"tasks\"][i][\"notebook_task\"]['notebook_path']\n",
    "        input_json[\"tasks\"][i][\"notebook_task\"]['notebook_path'] = solacc_path + \"/\" + notebook_name\n",
    "        \n",
    "    if \"job_clusters\" in input_json:\n",
    "      for j, _ in enumerate(input_json[\"job_clusters\"]):\n",
    "        if \"new_cluster\" in input_json[\"job_clusters\"][j]:\n",
    "          node_type_id_dict = input_json[\"job_clusters\"][j][\"new_cluster\"][\"node_type_id\"]\n",
    "          input_json[\"job_clusters\"][j][\"new_cluster\"][\"node_type_id\"] = node_type_id_dict[cloud]\n",
    "          if cloud == \"AWS\": \n",
    "            input_json[\"job_clusters\"][j][\"new_cluster\"][\"aws_attributes\"] = {\n",
    "                              \"availability\": \"ON_DEMAND\",\n",
    "                              \"zone_id\": \"auto\"\n",
    "                          }\n",
    "          if cloud == \"MSA\": \n",
    "            input_json[\"job_clusters\"][j][\"new_cluster\"][\"azure_attributes\"] = {\n",
    "                              \"availability\": \"ON_DEMAND_AZURE\",\n",
    "                              \"zone_id\": \"auto\"\n",
    "                          }\n",
    "          if cloud == \"GCP\": \n",
    "            input_json[\"job_clusters\"][j][\"new_cluster\"][\"gcp_attributes\"] = {\n",
    "                              \"use_preemptible_executors\": False\n",
    "                          }\n",
    "      input_json[\"access_control_list\"] = [\n",
    "          {\n",
    "          \"group_name\": \"users\",\n",
    "          \"permission_level\": \"CAN_MANAGE_RUN\"\n",
    "          }\n",
    "      ]\n",
    "    return input_json\n",
    "  \n",
    "  @staticmethod\n",
    "  def customize_pipeline_json(input_json, solacc_path):\n",
    "    for i, _ in enumerate(input_json[\"libraries\"]):\n",
    "      notebook_name = input_json[\"libraries\"][i][\"notebook\"]['path']\n",
    "      input_json[\"libraries\"][i][\"notebook\"]['path'] = solacc_path + \"/\" + notebook_name\n",
    "    return input_json\n",
    "  \n",
    "  def start_cluster(self, cluster_id):\n",
    "    \"starts cluster if terminated; no op otherwise\"\n",
    "    cluster_state = self.client.execute_get_json(f\"{self.client.endpoint}/api/2.0/clusters/get?cluster_id={cluster_id}\")[\"state\"]\n",
    "    if cluster_state in (\"TERMINATED\"):\n",
    "      response = self.client.execute_post_json(f\"{self.client.endpoint}/api/2.0/clusters/start\", {\"cluster_id\": cluster_id})\n",
    "      assert response == {}, \"\" # returns {} if 200\n",
    "      return\n",
    "     \n",
    "  \n",
    "  def install_libraries(self, jcid, jcl):\n",
    "    \"\"\"install_libraries is not synchronous: does not block until installs complete\"\"\" \n",
    "    self.client.execute_post_json(f\"{self.client.endpoint}/api/2.0/libraries/install\", {\"cluster_id\": jcid, \"libraries\":jcl} )\n",
    "    \n",
    "  @staticmethod\n",
    "  def get_library_list_for_cluster(job_input_json, jck):\n",
    "    jcl = []\n",
    "    for t in job_input_json[\"tasks\"]:\n",
    "      if \"job_cluster_key\" in t: # task such as DLT pipelines may not include a job cluster key\n",
    "        if t[\"job_cluster_key\"] == jck and \"libraries\" in t:\n",
    "          if t[\"libraries\"]:\n",
    "            jcl += t[\"libraries\"]\n",
    "    return jcl\n",
    "  \n",
    "  def set_acl_for_cluster(self, jcid):\n",
    "    response = self.client.execute_patch_json(f\"{self.client.endpoint}/api/2.0/preview/permissions/clusters/{jcid}\", \n",
    "                          {\n",
    "                            \"access_control_list\": [\n",
    "                              {\n",
    "                                \"group_name\": \"users\",\n",
    "                                \"permission_level\": \"CAN_RESTART\"\n",
    "                              }\n",
    "                            ]\n",
    "                          })\n",
    "\n",
    "  \n",
    "  def deploy_compute(self, input_json, run_job=False, wait=0):\n",
    "    self.job_input_json = copy.deepcopy(input_json)\n",
    "    self.job_params = self.customize_job_json(self.job_input_json, self.job_name, self.solacc_path, self.cloud)\n",
    "    self.job_id = self.create_or_update_job_by_name(self.job_params)\n",
    "    time.sleep(wait) # adding wait (seconds) to allow time for JSL cluster configuration using Partner Connect to complete\n",
    "    if not run_job: # if we don't run job, create interactive cluster\n",
    "      if \"job_clusters\" in self.job_params:\n",
    "        for job_cluster_params in self.job_params[\"job_clusters\"]:\n",
    "          jck = job_cluster_params[\"job_cluster_key\"]\n",
    "          if \"new_cluster\" in job_cluster_params:\n",
    "            jcid = self.create_or_update_cluster_by_name(self.convert_job_cluster_to_cluster(job_cluster_params)) # returns cluster id\n",
    "            self.set_acl_for_cluster(jcid)\n",
    "            jcl = self.get_library_list_for_cluster(self.job_input_json, jck)\n",
    "            if jcl:\n",
    "              self.start_cluster(jcid)\n",
    "              self.install_libraries(jcid, jcl)\n",
    "    else:\n",
    "      self.run_job()\n",
    "      \n",
    "  def deploy_pipeline(self, input_json, dlt_config_table, spark):\n",
    "    self.pipeline_input_json = copy.deepcopy(input_json)\n",
    "    self.pipeline_params = self.customize_pipeline_json(self.pipeline_input_json, self.solacc_path)\n",
    "    pipeline_name = self.pipeline_params[\"name\"] \n",
    "    return self.create_or_update_pipeline_by_name(dlt_config_table, pipeline_name, self.pipeline_params, spark) \n",
    "\n",
    "  def get_wsfs_folder_id(self, target_wsfs_directory): # Try creating a wsfs folder, return object id \n",
    "    trial = 1\n",
    "    client = self.client\n",
    "    try: \n",
    "      client.execute_post_json(f\"{client.endpoint}/api/2.0/workspace/mkdirs\", {\"path\": target_wsfs_directory})\n",
    "    except:\n",
    "      pass\n",
    "    wsfs_status = client.execute_get_json(f\"{client.endpoint}/api/2.0/workspace/get-status?path={target_wsfs_directory}\")\n",
    "    if wsfs_status[\"object_type\"] == \"DIRECTORY\":\n",
    "      return wsfs_status[\"object_id\"]\n",
    "    while wsfs_status[\"object_type\"] != \"DIRECTORY\":\n",
    "      trial += 1\n",
    "      try:\n",
    "        client.execute_post_json(f\"{client.endpoint}/api/2.0/workspace/mkdirs\", {\"path\": f\"{target_wsfs_directory}_{trial}\"})\n",
    "      except:\n",
    "        pass\n",
    "      wsfs_status = client.execute_get_json(f\"{client.endpoint}/api/2.0/workspace/get-status?path={target_wsfs_directory}_{trial}\")\n",
    "    return wsfs_status[\"object_id\"]\n",
    "\n",
    "  def check_if_dashboard_exists(self, id):\n",
    "    try:\n",
    "      self.client.execute_get_json(f\"{self.client.endpoint}/api/2.0/preview/sql/permissions/dashboards/{id}\")\n",
    "      return True\n",
    "    except Exception:\n",
    "      return False\n",
    "\n",
    "  def deploy_dbsql(self, input_path, dbsql_config_table, spark, reuse=True):\n",
    "    error_string = \"Cannot import dashboard; please enable dashboard import feature first\"\n",
    "    db, tb =  dbsql_config_table.split(\".\")\n",
    "    dbsql_config_table_exists = tb in [t.name for t in spark.catalog.listTables(db)]\n",
    "    dbsql_file_name = input_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    target_wsfs_directory = f\"\"\"/Users/{self.username}/{dbsql_file_name}\"\"\"\n",
    "    \n",
    "    # Try retrieve dashboard id if exists\n",
    "    if not reuse:\n",
    "      print(f\"Not reusing exisitng dashboards; a new dashboard will be created and the {dbsql_config_table} will include the new dashboard id\")\n",
    "      id = None \n",
    "    elif not dbsql_config_table_exists:\n",
    "      print(f\"{dbsql_config_table} does not exist\")\n",
    "      id = None \n",
    "    else:\n",
    "      dbsql_id_pdf = spark.table(dbsql_config_table).filter(f\"path = '{input_path}' and solacc = '{self.solacc_path}'\").toPandas()\n",
    "      assert len(dbsql_id_pdf) <= 1, f\"Two or more dashboards created from the same in-repo-path {input_path} exist in the {dbsql_config_table} table for the same accelerator {self.solacc_path}; this is unexpected; please remove the duplicative record(s) in {dbsql_config_table} and try again\"\n",
    "      id = dbsql_id_pdf['id'][0] if len(dbsql_id_pdf) > 0 else None\n",
    "      \n",
    "\n",
    "    # If we found the dashboard record in our table, and the dashboard was successfully created, then display the dashboard link and return id\n",
    "    if id and id != error_string and self.check_if_dashboard_exists(id):\n",
    "      if self.print_html:\n",
    "            displayHTML(f\"\"\"Found <a href=\"/sql/dashboards/{id}\" target=\"_blank\">DBSQL dashboard</a> created from {input_path} of this accelerator\"\"\")\n",
    "      else:\n",
    "            print(f\"\"\"Found dashboard for this accelerator at: {self.workspace_url}/sql/dashboards/{id}\"\"\")\n",
    "      return id\n",
    "    else:\n",
    "      # If the dashboard does not exist in record, or does not exist in the workspace, or we do not want to reuse it, create the dashboard first and log it to the dbsql table\n",
    "      # TODO: Remove try except once the API is in public preview\n",
    "      try:\n",
    "        # get the folder id for the folder we will save queries to\n",
    "        folder_object_id = self.get_wsfs_folder_id(target_wsfs_directory)\n",
    "\n",
    "        # create dashboard\n",
    "        with open(input_path) as f:\n",
    "          input_json = json.load(f)\n",
    "        client = self.client\n",
    "        result = client.execute_post_json(f\"{client.endpoint}/api/2.0/preview/sql/dashboards/import\", {'parent': f'folders/{folder_object_id}', \"import_file_contents\": input_json})\n",
    "        id = result['id']\n",
    "        \n",
    "        # create record in dbsql table to enable reuse\n",
    "        if not dbsql_config_table_exists:\n",
    "          # initialize table\n",
    "          spark.createDataFrame([{\"path\": input_path, \"id\": id, \"solacc\": self.solacc_path}]).write.mode(\"append\").option(\"mergeSchema\", \"True\").saveAsTable(dbsql_config_table)\n",
    "        else:\n",
    "          # upsert table record\n",
    "          spark.createDataFrame([{\"path\": input_path, \"id\": id, \"solacc\": self.solacc_path}]).createOrReplaceTempView(\"new_record\")\n",
    "          spark.sql(f\"\"\"MERGE INTO {dbsql_config_table} t USING new_record n\n",
    "          ON t.path = n.path and t.solacc = n.solacc\n",
    "          WHEN MATCHED THEN UPDATE SET t.id = n.id\n",
    "          WHEN NOT MATCHED THEN INSERT *\n",
    "          \"\"\")\n",
    "        \n",
    "        # display result\n",
    "        if self.print_html:\n",
    "            displayHTML(f\"\"\"Created <a href=\"/sql/dashboards/{id}\" target=\"_blank\">{result['name']} dashboard</a> \"\"\")\n",
    "        else:\n",
    "            print(f\"\"\"Created {result['name']} dashboard at: {self.workspace_url}/sql/dashboards/{id}-{result['slug']}\"\"\")\n",
    "        \n",
    "        return id\n",
    "      \n",
    "      except:\n",
    "        pass\n",
    "    \n",
    "  def submit_run(self, task_json):\n",
    "    json_response = self.client.execute_post_json(f\"/2.1/jobs/runs/submit\", task_json)\n",
    "    assert \"run_id\" in json_response, \"task_json submission errored\"\n",
    "    run_id = json_response[\"run_id\"]\n",
    "    response = self.client.runs().wait_for(run_id)\n",
    "    result_state= response['state'].get('result_state', None)\n",
    "    assert result_state == \"SUCCESS\", f\"Run failed; please investigate at: {self.workspace_url}#job/<job_id>/run/{run_id} where the <job_id> is the part before `-` on the printed output above\" \n",
    "\n",
    "  def run_job(self):\n",
    "    self.run_id = self.client.jobs().run_now(self.job_id)[\"run_id\"]\n",
    "    response = self.client.runs().wait_for(self.run_id)\n",
    "    \n",
    "    # print info about result state\n",
    "    self.test_result_state= response['state'].get('result_state', None)\n",
    "    self.life_cycle_state = response['state'].get('life_cycle_state', None)\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"#job/{self.job_id}/run/{self.run_id} is {self.life_cycle_state} - {self.test_result_state}\")\n",
    "    assert self.test_result_state == \"SUCCESS\", f\"Job Run failed: please investigate at: {self.workspace_url}#job/{self.job_id}/run/{self.run_id}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ed7983b-02ab-46d2-b7b8-7e1a4014fff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "solacc_companion_init",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
