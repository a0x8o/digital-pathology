# Databricks notebook source
# MAGIC %md
# MAGIC You may find this series of notebooks at https://github.com/databricks-industry-solutions/digital-pathology. For more information about this solution accelerator, visit https://www.databricks.com/solutions/accelerators/digital-pathology.

# COMMAND ----------

# MAGIC %md
# MAGIC # Automating Digital Pathology <img src="https://databricks.com/wp-content/themes/databricks/assets/images/header_logo_2x.png" alt="logo" width="250"/> 
# MAGIC 
# MAGIC ## Overview
# MAGIC 
# MAGIC The tumor proliferation speed or tumor growth is an important biomarker for predicting patient outcomes. Proper assessment of this biomarker is crucial for informing the decisions for the treatment plan for the patient. In a clinical setting, the most common method is to count [mitotic figures](https://www.mypathologyreport.ca/mitotic-figure/#:~:text=A%20mitotic%20figure%20is%20a,at%20tissue%20under%20the%20microscope) under a microscope by a pathologist. The manual counting and subjectivity of the process pose a reproducibility challenge. This has been the main motivation for many efforts to automate this process and use advanced ML techniques.
# MAGIC 
# MAGIC 
# MAGIC One of the main challenges however for automating this task, is the fact that whole slide images are rather large. WSI images can vary anywhere between 0.5 to 3.5GB in size, and that can slow down the image preprocessing step which is necessary for any downstream ML application.
# MAGIC 
# MAGIC 
# MAGIC In this solution accelerator, we walk you through a step-by-step process to use databricks capabilities to perform image segmentation and pre-processing on WSI and train a binary classifier that produces a metastasis probability map over a whole slide image (WSI).
# MAGIC 
# MAGIC <br>
# MAGIC <img src="https://hls-eng-data-public.s3.amazonaws.com/img/slide_heatmap.png" alt="logo" width=60% /> 
# MAGIC </br>
# MAGIC 
# MAGIC ## Dataset
# MAGIC The data used in this solution accelerator is from the [Camelyon16 Grand Challenge](http://gigadb.org/dataset/100439), along with annotations based on hand-drawn metastasis outlines. We use curated annotations for this dataset obtained from Baidu Research [github repository](https://github.com/baidu-research/NCRF).
# MAGIC 
# MAGIC 
# MAGIC ## Notebooks
# MAGIC We use Apache Spark's parallelization capabilities, using pandas_udf, to generate tumor/normal patches based on annotation data as well as feature extraction, using a pre-trained [InceptionV3](https://keras.io/api/applications/inceptionv3/). We use the embeddings obtained this way to explore clusters of pacthes by visualizing 2d and 3d embeddings, using UMAP. We then use transfer learning with pytorch to train a convnet to classify tumor vs normal patches and later use the resulting model to overlay a metastasis heatmap on a new slide.
# MAGIC 
# MAGIC 
# MAGIC This solution accelerator contains the following notebooks:
# MAGIC 
# MAGIC - `config`: configuring paths and other settings. Also for the first time setting up a cluster for patch generation, use the `initscript` generated by the config notebook to install `openSlide` on your cluster.
# MAGIC 
# MAGIC - `1-create-annotation-deltalke`: to download annotations and write to delta.
# MAGIC 
# MAGIC - `2-patch-generation`: This notebook generates patches from WSI based on annoations. 
# MAGIC 
# MAGIC - `3-feature-extraction`: To extract image embeddings using `InceptionV3` in a distributed manner
# MAGIC 
# MAGIC - `4-unsupervised-learning`: dimensionality reduction and cluster inspection with UMAP
# MAGIC 
# MAGIC - `5-training`: In this notebook we tune and train a binary classifier to classify tumor/normal patches with pytorch and log the model with mlflow.
# MAGIC 
# MAGIC 
# MAGIC - `6-metastasis-heatmap`: This notebook we use the model trained in the previous step to generate a metastasis probability heatmap for a given slide.
# MAGIC 
# MAGIC 
# MAGIC - `definitions`: This notebook contains definitions for some of the functions that are used in multiple places (for example patch generation and pre processing)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Cluster Setup for patch generation
# MAGIC 
# MAGIC For the first time running this workflow, you need to create an `Init Script` to install `openlide-tools` from [OpenSlide library](https://openslide.org/) on your cluster. By running the configuration notebook this initscript is generated and you only need to specify the path `/tmp/openslide/openslide-tools.sh` 
# MAGIC 
# MAGIC This script then should be [attached to the cluster we are using](https://docs.databricks.com/user-guide/clusters/init-scripts.html#configure-a-cluster-scoped-init-script).
# MAGIC 
# MAGIC In your cluster's `Init Script` configuration pane, add the path `/tmp/openslide/openslide-tools.sh`. 
# MAGIC After doing so, **re-start your cluster** and attached this notebook to the cluster before running the rest of the commands.
# MAGIC After the cluster re-starts, go to Libraries tab on your cluster configuration and install **`openslide-python`** from PyPi.
# MAGIC 
# MAGIC You can use the same cluster for notebooks `2,3` and `4`. 
# MAGIC 
# MAGIC ## Training the Model
# MAGIC To train the model you can either use cpu or gpu cluster (faster training with GPU). We have tested the training on 5000 images on a single node `g4dn.4xlarge` with 64MB memory (recommended) which took ~3.5 min with 4 epochs. for larger number of images you can use [distributed methods for training your model](https://docs.databricks.com/applications/machine-learning/train-model/distributed-training/index.html#distributed-training)

# COMMAND ----------

slides_html="""
<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vQkRfYrJsS_2s73lc9Z5Pz15E6r8FhnRVaeWc49VjW1NfHmoGzxoE1GfyJDY4b7dfu7BMQ99X6nlLzp/embed?start=true&loop=true&delayms=3000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
"""
displayHTML(slides_html)

# COMMAND ----------

# DBTITLE 1,WSI dataset
WSI_PATH='/databricks-datasets/med-images/camelyon16/'
display(dbutils.fs.ls(WSI_PATH))

# COMMAND ----------

# MAGIC %md
# MAGIC after creating the cluster, attach this notebook and run this command for a quick look at the slides and test if the libraries are installed.

# COMMAND ----------

# DBTITLE 1,Quick view of some of the slides
import numpy as np
import openslide
import matplotlib.pyplot as plt

f, axarr = plt.subplots(1,4,sharey=True)
i=0
for pid in ["normal_034","normal_036","tumor_044", "tumor_045"]:
  path = f'/dbfs/{WSI_PATH}/{pid}.tif'
  slide = openslide.OpenSlide(path)
  axarr[i].imshow(slide.get_thumbnail([m//50 for m in slide.dimensions]))
  axarr[i].set_title(pid)
  i+=1

# COMMAND ----------

# DBTITLE 1,Viewing slides at different zoom levels
sid='tumor_049'
slide = openslide.OpenSlide(f'/dbfs/{WSI_PATH}/{sid}.tif')
image_datas=[]
region=[14793,127384]
size=[1000,1000]
f, axarr = plt.subplots(1,3,sharex=True,sharey=True)
for level,ind in zip([0,4,5],[0,1,2]):
  img = slide.read_region(region,level,size)
  axarr[ind].imshow(img)
  axarr[ind].set_title(f"level:{level}")
display()

# COMMAND ----------

# MAGIC %md
# MAGIC ## License
# MAGIC Copyright / License info of the notebook. Copyright [2021] the Notebook Authors.  The source in this notebook is provided subject to the [Apache 2.0 License](https://spdx.org/licenses/Apache-2.0.html).  All included or referenced third party libraries are subject to the licenses set forth below.
# MAGIC 
# MAGIC |Library Name|Library License|Library License URL|Library Source URL| 
# MAGIC | :-: | :-:| :-: | :-:|
# MAGIC |Pandas |BSD 3-Clause License| https://github.com/pandas-dev/pandas/blob/master/LICENSE | https://github.com/pandas-dev/pandas|
# MAGIC |Numpy |BSD 3-Clause License| https://github.com/numpy/numpy/blob/main/LICENSE.txt | https://github.com/numpy/numpy|
# MAGIC |Apache Spark |Apache License 2.0| https://github.com/apache/spark/blob/master/LICENSE | https://github.com/apache/spark/tree/master/python/pyspark|
# MAGIC |Pillow (PIL) | HPND License| https://github.com/python-pillow/Pillow/blob/master/LICENSE | https://github.com/python-pillow/Pillow/|
# MAGIC |OpenSlide | GNU LGPL version 2.1 |https://github.com/openslide/openslide/blob/main/LICENSE.txt| https://github.com/openslide| 
# MAGIC |Open Slide Python| GNU LGPL version 2.1 |https://github.com/openslide/openslide-python/blob/main/LICENSE.txt| https://github.com/openslide/openslide-python|
# MAGIC |pytorch lightning|Apache License 2.0| https://github.com/PyTorchLightning/pytorch-lightning/blob/master/LICENSE | https://github.com/PyTorchLightning/pytorch-lightning|
# MAGIC |NCRF|Apache License 2.0|https://github.com/baidu-research/NCRF/blob/master/LICENSE|https://github.com/baidu-research/NCRF|
# MAGIC 
# MAGIC |Author|
# MAGIC |-|
# MAGIC |Databricks Inc.|

# COMMAND ----------

# MAGIC %md
# MAGIC ## Disclaimers
# MAGIC Databricks Inc. (“Databricks”) does not dispense medical, diagnosis, or treatment advice. This Solution Accelerator (“tool”) is for informational purposes only and may not be used as a substitute for professional medical advice, treatment, or diagnosis. This tool may not be used within Databricks to process Protected Health Information (“PHI”) as defined in the Health Insurance Portability and Accountability Act of 1996, unless you have executed with Databricks a contract that allows for processing PHI, an accompanying Business Associate Agreement (BAA), and are running this notebook within a HIPAA Account.  Please note that if you run this notebook within Azure Databricks, your contract with Microsoft applies.
