{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de8466cf-b0fc-41a8-ae48-f6b83444b8d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Digital Pathology `NotebookSolutionCompanion`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3127bc5-f8d9-4c5c-a6cb-8fecac837a15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Instantiate `NotebookSolutionCompanion` from `./solacc_companion_init`**\n",
    "  <!-- - The `__init__.py` code used here is taken from a prior PR that works without throwing the observed `request` error e.g. [solacc/companion/\\__init\\__.py](https://github.com/databricks-industry-solutions/notebook-solution-companion/blob/f7e381d77675b29c2d3f9d377a528ceaf2255f23/solacc/companion/__init__.py) link wrt the PR update -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "549819eb-33b9-4008-a03e-15e86fb727d2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run copied solacc companion module from workspace path"
    }
   },
   "outputs": [],
   "source": [
    "%run ./solacc_companion_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0aede704-45f4-41aa-84e8-06783f0e279b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Access Modules within NotebookSolutionCompanion()"
    }
   },
   "outputs": [],
   "source": [
    "nsc = NotebookSolutionCompanion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e9282de-84d4-424b-8af0-d3491968a6f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Add Additional Models to `NotebookSolutionCompanion`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99c77833-1bb6-46f2-a8c3-3288cfb3d3bf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add DigiPath methods to the NotebookSolutionCompanion instance"
    }
   },
   "outputs": [],
   "source": [
    "## To consider being included within NotebookSolutionCompanion (nsc) in the future -- for now this is a workaround\n",
    "\n",
    "# Methods to add to the current instance of NotebookSolutionCompanion (nsc)\n",
    "def _nsc_deploy_job_clusters(self, job_clusters_json, reuse=True, wait=0):\n",
    "    \"\"\"\n",
    "    Deploy only the job clusters defined in the job configuration.\n",
    "    Returns a dictionary mapping job_cluster_keys to their corresponding cluster IDs.\n",
    "    \"\"\"\n",
    "    job_cluster_map = {}\n",
    "    \n",
    "    if not job_clusters_json:\n",
    "        print(\"No job clusters to deploy\")\n",
    "        return job_cluster_map\n",
    "    \n",
    "    print(f\"üöÄ Deploying {len(job_clusters_json)} job clusters...\")\n",
    "    \n",
    "    for job_cluster_params in job_clusters_json:\n",
    "        jck = job_cluster_params[\"job_cluster_key\"]\n",
    "        if \"new_cluster\" in job_cluster_params:\n",
    "            # Convert job cluster config to interactive cluster config\n",
    "            cluster_params = self.convert_job_cluster_to_cluster(job_cluster_params)\n",
    "            \n",
    "            # Set auto-termination to 10 minutes\n",
    "            cluster_params[\"autotermination_minutes\"] = 10\n",
    "            \n",
    "            # Apply cloud-specific customization to the cluster parameters\n",
    "            cluster_params = self.customize_cluster_json(cluster_params)\n",
    "            \n",
    "            # Check if cluster with this name already exists\n",
    "            cluster_name = cluster_params[\"cluster_name\"]\n",
    "            clusters = self.client.execute_get_json(f\"{self.client.endpoint}/api/2.0/clusters/list\")[\"clusters\"]\n",
    "            clusters_matched = list(filter(lambda cluster: cluster_name == cluster[\"cluster_name\"], clusters))\n",
    "            cluster_exists = len(clusters_matched) > 0\n",
    "            \n",
    "            if cluster_exists and reuse:\n",
    "                # Reuse the existing cluster\n",
    "                cluster_id = clusters_matched[0][\"cluster_id\"]\n",
    "                print(f\"‚úÖ Reusing existing cluster '{cluster_name}' with ID: {cluster_id}\")\n",
    "            else:\n",
    "                if cluster_exists and not reuse:\n",
    "                    # Delete the existing cluster first\n",
    "                    cluster_id = clusters_matched[0][\"cluster_id\"]\n",
    "                    print(f\"üóëÔ∏è Deleting existing cluster '{cluster_name}' with ID: {cluster_id}\")\n",
    "                    self.client.execute_post_json(f\"{self.client.endpoint}/api/2.0/clusters/permanent-delete\", {\"cluster_id\": cluster_id})\n",
    "                    time.sleep(5)  # Wait a bit for the deletion to take effect\n",
    "                \n",
    "                # Create or update the cluster\n",
    "                jcid = self.create_or_update_cluster_by_name(cluster_params)\n",
    "                \n",
    "                # Set ACL for the cluster\n",
    "                self.set_acl_for_cluster(jcid)\n",
    "                \n",
    "                cluster_id = jcid\n",
    "            \n",
    "            # Store the mapping\n",
    "            job_cluster_map[jck] = cluster_id\n",
    "            \n",
    "            # Get libraries for this job cluster\n",
    "            if hasattr(self, 'job_input_json'):\n",
    "                jcl = self.get_library_list_for_cluster(self.job_input_json, jck)\n",
    "                if jcl:\n",
    "                    self.start_cluster(cluster_id)\n",
    "                    self.install_libraries(cluster_id, jcl)\n",
    "    \n",
    "    time.sleep(wait)\n",
    "    print(f\"‚úÖ Successfully deployed {len(job_cluster_map)} job clusters\")\n",
    "    return job_cluster_map\n",
    "\n",
    "\n",
    "def _nsc_deploy_job_with_existing_clusters(self, job_json, cluster_map, reuse=True, run_job=False):\n",
    "    \"\"\"\n",
    "    Deploy a job using existing clusters instead of job clusters.\n",
    "    \"\"\"\n",
    "    # Create a deep copy of the job configuration\n",
    "    job_params = copy.deepcopy(job_json)\n",
    "    \n",
    "    # Customize the notebook paths in the job JSON\n",
    "    for i, task in enumerate(job_params.get(\"tasks\", [])):\n",
    "        if \"notebook_task\" in task:\n",
    "            notebook_name = task[\"notebook_task\"][\"notebook_path\"]\n",
    "            if not notebook_name.startswith(self.solacc_path):\n",
    "                task[\"notebook_task\"][\"notebook_path\"] = f\"{self.solacc_path}/{notebook_name}\"\n",
    "    \n",
    "    # Set the job name if not already set\n",
    "    if \"name\" not in job_params:\n",
    "        job_params[\"name\"] = self.job_name\n",
    "    \n",
    "    # Add access control list\n",
    "    job_params[\"access_control_list\"] = [\n",
    "        {\n",
    "            \"group_name\": \"users\",\n",
    "            \"permission_level\": \"CAN_MANAGE_RUN\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Remove the job_clusters section\n",
    "    if \"job_clusters\" in job_params:\n",
    "        del job_params[\"job_clusters\"]\n",
    "    \n",
    "    # Replace job_cluster_key with existing_cluster_id in tasks\n",
    "    for task in job_params.get(\"tasks\", []):\n",
    "        if \"job_cluster_key\" in task and task[\"job_cluster_key\"] in cluster_map:\n",
    "            task[\"existing_cluster_id\"] = cluster_map[task[\"job_cluster_key\"]]\n",
    "            del task[\"job_cluster_key\"]\n",
    "    \n",
    "    # Check if job with this name already exists\n",
    "    job_name = job_params[\"name\"]\n",
    "    job_found = list(self.w.jobs.list(name=job_name))\n",
    "    job_exists = len(job_found) > 0\n",
    "    \n",
    "    if job_exists and reuse:\n",
    "        # Reuse the existing job by updating it\n",
    "        job_id = job_found[0].job_id\n",
    "        print(f\"‚úÖ Updating existing job '{job_name}' with ID: {job_id}\")\n",
    "        reset_job_settings = JobSettings().from_dict(job_params)\n",
    "        self.w.jobs.reset(job_id, reset_job_settings)\n",
    "    else:\n",
    "        if job_exists and not reuse:\n",
    "            # Delete the existing job first\n",
    "            job_id = job_found[0].job_id\n",
    "            print(f\"üóëÔ∏è Deleting existing job '{job_name}' with ID: {job_id}\")\n",
    "            self.w.jobs.delete(job_id=job_id)\n",
    "            time.sleep(5)  # Wait a bit for the deletion to take effect\n",
    "        \n",
    "        # Create a new job\n",
    "        create_job_request = CreateJob().from_dict(job_params)\n",
    "        job_id = self.w.jobs.create(request=create_job_request).job_id\n",
    "        print(f\"‚úÖ Created new job '{job_name}' with ID: {job_id}\")\n",
    "    \n",
    "    # Store the job ID for future reference\n",
    "    self.job_id = job_id\n",
    "    \n",
    "    # Run the job if requested\n",
    "    run_id = None\n",
    "    if run_job:\n",
    "        print(f\"üöÄ Running job '{job_name}' with ID: {job_id}\")\n",
    "        # Use the Databricks SDK directly instead of self.run_job()\n",
    "        run_response = self.w.jobs.run_now(job_id=job_id)\n",
    "        run_id = run_response.run_id\n",
    "    \n",
    "    return job_id, run_id\n",
    "\n",
    "\n",
    "def _nsc_deploy_digital_pathology_job(self, job_json, suffix=\"\", reuse=True, run_job=False):\n",
    "    \"\"\"\n",
    "    Deploy a digital pathology job using the two-step approach:\n",
    "    1. Deploy the job clusters separately\n",
    "    2. Deploy the job with references to the deployed clusters\n",
    "    \"\"\"\n",
    "    # Store the job JSON for use in other methods\n",
    "    self.job_input_json = copy.deepcopy(job_json)\n",
    "    \n",
    "    # Step 1: Deploy the job clusters separately\n",
    "    job_clusters = job_json.get(\"job_clusters\", [])\n",
    "    cluster_map = self._deploy_job_clusters(job_clusters, reuse=reuse)\n",
    "    \n",
    "    # Step 2: Deploy the job with references to the deployed clusters\n",
    "    job_id, run_id = self._deploy_job_with_existing_clusters(job_json, cluster_map, reuse=reuse, run_job=run_job)\n",
    "    \n",
    "    # Get the job name for reference\n",
    "    job_name = job_json.get(\"name\", \"digital-pathology-job\")\n",
    "    \n",
    "    # Collect cluster names and IDs\n",
    "    cluster_details = {}\n",
    "    for key, cluster_id in cluster_map.items():\n",
    "        # Get the cluster name from the job_clusters configuration\n",
    "        cluster_name = key\n",
    "        for jc in job_clusters:\n",
    "            if jc[\"job_cluster_key\"] == key:\n",
    "                # Use the cluster name from the job_cluster_key if available\n",
    "                cluster_name = jc[\"job_cluster_key\"]\n",
    "                break\n",
    "        \n",
    "        cluster_details[key] = {\n",
    "            \"cluster_id\": cluster_id,\n",
    "            \"cluster_name\": cluster_name\n",
    "        }\n",
    "    \n",
    "    # Return comprehensive result\n",
    "    result = {\n",
    "        \"job_id\": job_id,\n",
    "        \"job_name\": job_name,\n",
    "        \"clusters\": cluster_map,\n",
    "        \"cluster_details\": cluster_details\n",
    "    }\n",
    "    \n",
    "    if run_id:\n",
    "        result[\"run_id\"] = run_id\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Add these methods to the existing instance of NotebookSolutionCompanion (nsc)\n",
    "import types\n",
    "nsc._deploy_job_clusters = types.MethodType(_nsc_deploy_job_clusters, nsc)\n",
    "nsc._deploy_job_with_existing_clusters = types.MethodType(_nsc_deploy_job_with_existing_clusters, nsc)\n",
    "nsc._deploy_digital_pathology_job = types.MethodType(_nsc_deploy_digital_pathology_job, nsc)\n",
    "\n",
    "print(\"‚úÖ Added new methods to existing instance of NotebookSolutionCompanion (nsc)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "258e5feb-4ed6-4c11-847a-17f2472aff52",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "+ cleanup_digital_pathology_resources --> nsc"
    }
   },
   "outputs": [],
   "source": [
    "# Define the cleanup method\n",
    "def _nsc_cleanup_digital_pathology_resources(self, results, confirm=True):\n",
    "    \"\"\"\n",
    "    Clean up resources (job and clusters) created during digital pathology job deployment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        The results dictionary returned by deploy_digital_pathology_job\n",
    "    confirm : bool, optional\n",
    "        Whether to ask for confirmation before deleting resources (default: True)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with deletion status for each resource\n",
    "    \"\"\"\n",
    "    if not isinstance(results, dict):\n",
    "        print(\"‚ùå Invalid results object. Please provide the dictionary returned by deploy_digital_pathology_job.\")\n",
    "        return {\"status\": \"failed\", \"reason\": \"Invalid results object\"}\n",
    "    \n",
    "    # Extract resource IDs\n",
    "    job_id = results.get('job_id')\n",
    "    clusters = results.get('clusters', {})\n",
    "    \n",
    "    if not job_id and not clusters:\n",
    "        print(\"‚ùå No resources to clean up. The results dictionary doesn't contain job_id or clusters.\")\n",
    "        return {\"status\": \"failed\", \"reason\": \"No resources found\"}\n",
    "    \n",
    "    # Ask for confirmation if required\n",
    "    if confirm:\n",
    "        print(f\"‚ö†Ô∏è You are about to delete the following resources:\")\n",
    "        if job_id:\n",
    "            print(f\"   - Job: {results.get('job_name', 'Unknown')} (ID: {job_id})\")\n",
    "        \n",
    "        if clusters:\n",
    "            print(f\"   - Clusters ({len(clusters)}):\")\n",
    "            for key, cluster_id in clusters.items():\n",
    "                print(f\"     - {key} (ID: {cluster_id})\")\n",
    "        \n",
    "        confirmation = input(\"\\nAre you sure you want to delete these resources? (y/n): \").strip().lower()\n",
    "        if confirmation != 'y':\n",
    "            print(\"‚ùå Cleanup cancelled.\")\n",
    "            return {\"status\": \"cancelled\"}\n",
    "    \n",
    "    deletion_status = {\"job\": None, \"clusters\": {}}\n",
    "    \n",
    "    # Delete the job\n",
    "    if job_id:\n",
    "        try:\n",
    "            print(f\"üóëÔ∏è Deleting job {results.get('job_name', 'Unknown')} (ID: {job_id})...\")\n",
    "            self.client.execute_post_json(f\"{self.client.endpoint}/api/2.0/jobs/delete\", {\"job_id\": job_id})\n",
    "            print(f\"‚úÖ Successfully deleted job with ID: {job_id}\")\n",
    "            deletion_status[\"job\"] = \"success\"\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to delete job {job_id}: {e}\")\n",
    "            deletion_status[\"job\"] = f\"failed: {str(e)}\"\n",
    "    \n",
    "    # Delete the clusters\n",
    "    for key, cluster_id in clusters.items():\n",
    "        try:\n",
    "            print(f\"üóëÔ∏è Deleting cluster {key} (ID: {cluster_id})...\")\n",
    "            self.client.execute_post_json(f\"{self.client.endpoint}/api/2.0/clusters/permanent-delete\", {\"cluster_id\": cluster_id})\n",
    "            print(f\"‚úÖ Successfully deleted cluster with ID: {cluster_id}\")\n",
    "            deletion_status[\"clusters\"][cluster_id] = \"success\"\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to delete cluster {cluster_id}: {e}\")\n",
    "            deletion_status[\"clusters\"][cluster_id] = f\"failed: {str(e)}\"\n",
    "    \n",
    "    # Overall status\n",
    "    if deletion_status[\"job\"] == \"success\" and all(status == \"success\" for status in deletion_status[\"clusters\"].values()):\n",
    "        deletion_status[\"status\"] = \"success\"\n",
    "        print(\"\\n‚úÖ All resources have been successfully deleted.\")\n",
    "    else:\n",
    "        deletion_status[\"status\"] = \"partial\"\n",
    "        print(\"\\n‚ö†Ô∏è Some resources could not be deleted. See details above.\")\n",
    "    \n",
    "    return deletion_status\n",
    "\n",
    "# Add the method to the existing instance of NotebookSolutionCompanion (nsc)\n",
    "nsc.cleanup_digital_pathology_resources = types.MethodType(_nsc_cleanup_digital_pathology_resources, nsc)\n",
    "\n",
    "print(\"‚úÖ Added cleanup method to NotebookSolutionCompanion (nsc) instance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b21f0daa-029f-44af-b599-e5d4677a21f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Include function to deploy workflow job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72c3564c-7a6e-4a73-a78b-49b488b877a3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "deploy_digital_pathology_job"
    }
   },
   "outputs": [],
   "source": [
    "## Function to deploy resources and job\n",
    "def deploy_digital_pathology_job(suffix=\"\", reuse=True, run_job=False, workspace_url=None):\n",
    "    \"\"\"\n",
    "    Creates and deploys the digital pathology job using the enhanced NSC methods\n",
    "    \"\"\"\n",
    "    # Create job configuration\n",
    "    job_json = create_digital_pathology_job_config(suffix)\n",
    "    \n",
    "    # Deploy the job using the new method\n",
    "    print(\"üöÄ Creating digital pathology job with job clusters...\")\n",
    "    result = nsc._deploy_digital_pathology_job(job_json, suffix, reuse, run_job)\n",
    "    \n",
    "    print(\"\\n‚úÖ Job deployment complete!\")\n",
    "    \n",
    "    # Add URLs if workspace_url is provided\n",
    "    if workspace_url:\n",
    "        # Ensure URL is properly formatted\n",
    "        if workspace_url.endswith(\"/\"):\n",
    "            workspace_url = workspace_url[:-1]  # Remove trailing slash if present\n",
    "        \n",
    "        # Add job URL\n",
    "        job_id = result['job_id']\n",
    "        job_name = result['job_name']\n",
    "        job_url = f\"{workspace_url}/#job/{job_id}\"\n",
    "        result['job_url'] = job_url\n",
    "        \n",
    "        # Add cluster URLs\n",
    "        cluster_urls = {}\n",
    "        for key, cluster_id in result['clusters'].items():\n",
    "            cluster_url = f\"{workspace_url}/#setting/clusters/{cluster_id}/configuration\"\n",
    "            cluster_urls[key] = cluster_url\n",
    "        \n",
    "        result[\"cluster_urls\"] = cluster_urls\n",
    "        \n",
    "        # Add run URL if a run was started\n",
    "        if \"run_id\" in result:\n",
    "            run_id = result['run_id']\n",
    "            job_run_url = f\"{workspace_url}/#job/{job_id}/run/{run_id}\"\n",
    "            result['job_run_url'] = job_run_url\n",
    "    \n",
    "    # # Display a summary of the deployment with links\n",
    "    # print(\"\\nüìã Deployment Summary:\")\n",
    "    \n",
    "    # # Display job information\n",
    "    # if workspace_url and 'job_url' in result:\n",
    "    #     job_link = f\"<a href='{result['job_url']}' target='_blank'>{result['job_name']}</a>\"\n",
    "    #     print(f\"   - Job: {job_link if nsc.print_html else result['job_name']} (ID: {result['job_id']})\")\n",
    "    #     if not nsc.print_html:\n",
    "    #         print(f\"   - Job URL: {result['job_url']}\")\n",
    "    # else:\n",
    "    #     print(f\"   - Job ID: {result['job_id']}\")\n",
    "    #     print(f\"   - Job Name: {result['job_name']}\")\n",
    "    \n",
    "    # # Display run information if available\n",
    "    # if \"run_id\" in result and workspace_url and 'job_run_url' in result:\n",
    "    #     run_link = f\"<a href='{result['job_run_url']}' target='_blank'>Run #{result['run_id']}</a>\"\n",
    "    #     print(f\"   - Run: {run_link if nsc.print_html else 'Run #' + str(result['run_id'])}\")\n",
    "    #     if not nsc.print_html:\n",
    "    #         print(f\"   - Run URL: {result['job_run_url']}\")\n",
    "    # elif \"run_id\" in result:\n",
    "    #     print(f\"   - Run ID: {result['run_id']}\")\n",
    "    \n",
    "    # # Display cluster information\n",
    "    # print(f\"   - Clusters: {len(result['clusters'])} deployed with 10-minute auto-termination\")\n",
    "    \n",
    "    # if workspace_url and \"cluster_urls\" in result:\n",
    "    #     print(\"   - Cluster details:\")\n",
    "    #     for key, cluster_id in result['clusters'].items():\n",
    "    #         cluster_url = result['cluster_urls'][key]\n",
    "    #         cluster_link = f\"<a href='{cluster_url}' target='_blank'>{key}</a>\"\n",
    "    #         print(f\"     - {cluster_link if nsc.print_html else key} (ID: {cluster_id})\")\n",
    "    #         if not nsc.print_html:\n",
    "    #             print(f\"       {cluster_url}\")\n",
    "    \n",
    "    # Use displayHTML for a more interactive display if supported\n",
    "    if nsc.print_html:\n",
    "        try:\n",
    "            from IPython.display import display, HTML\n",
    "            \n",
    "            # Create HTML for job information\n",
    "            job_html = f\"\"\"\n",
    "            <div style=\"margin-top: 20px; padding: 10px; border: 1px solid #ccc; border-radius: 5px; background-color: #f8f8f8;\">\n",
    "                <h3 style=\"margin-top: 0;\">Digital Pathology Job Deployment</h3>\n",
    "                <p><strong>Job:</strong> <a href=\"{result['job_url']}\" target=\"_blank\">{result['job_name']}</a> (ID: {result['job_id']})</p>\n",
    "            \"\"\"\n",
    "            \n",
    "            # Add run information if available\n",
    "            if \"run_id\" in result and 'job_run_url' in result:\n",
    "                job_html += f\"\"\"<p><strong>Run:</strong> <a href=\"{result['job_run_url']}\" target=\"_blank\">Run #{result['run_id']}</a></p>\"\"\"\n",
    "            \n",
    "            # Add cluster information\n",
    "            job_html += \"\"\"<p><strong>Clusters:</strong></p><ul>\"\"\"\n",
    "            for key, cluster_id in result['clusters'].items():\n",
    "                cluster_url = result['cluster_urls'][key]\n",
    "                job_html += f\"\"\"<li><a href=\"{cluster_url}\" target=\"_blank\">{key}</a> (ID: {cluster_id})</li>\"\"\"\n",
    "            job_html += \"\"\"</ul></div>\"\"\"\n",
    "            \n",
    "            display(HTML(job_html))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_c5986e9d-37a9-4446-a259-103b92642bc0",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "solacc_companion_digipath",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
